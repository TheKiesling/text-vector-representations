{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242264a7",
   "metadata": {},
   "source": [
    "# Text Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce3a1c",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02e67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783880b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04594e63",
   "metadata": {},
   "source": [
    "Según la documentación del dataset ([sklearn.datasets.fetch_20newsgroups](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)) hay parámetros los cuales se pueden usar estratégicamente antes del procesamiento y ahorrar operaciones. Para delimitar las categorías indicadas en el laboratorio, se tienen que encontrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5b892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d29a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"talk.politics.guns\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"rec.autos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e975a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d053515",
   "metadata": {},
   "source": [
    "Además, como se puede ver, los elementos del dataset son correos, por lo que se puede hacer uso del parámetro `remove` para eliminar las líneas no deseadas y quedarnos solo con el cuerpo del mensaje. Esta decisión se toma ya que no se quiere que el modelo entienda que la palabra \"From:\" y un correo tienen relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7346c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_content = (\n",
    "    \"headers\",\n",
    "    \"footers\",\n",
    "    \"quotes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7631b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset=\"all\", categories=categories, remove=remove_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d596dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877575",
   "metadata": {},
   "source": [
    "## Preprocesamiento del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4f708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_preprocessed = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84daa0df",
   "metadata": {},
   "source": [
    "### Eliminación de caracteres no alfabéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93927f",
   "metadata": {},
   "source": [
    "Dado que hay caracteres no alfabéticos en el corpus, se debe implementar una función para eliminarlos. Esta función tomará un texto como entrada y devolverá el texto procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95c64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    return ''.join(c for c in text if c.isalpha() or c.isspace())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73cb11",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48ca09",
   "metadata": {},
   "source": [
    "Para la estandarización de las palabras, se pasará todo el texto a minúsculas. Esto ayudará a reducir la variabilidad en la representación de las palabras y facilitará el procesamiento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a9e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b36359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text):\n",
    "    text = remove_non_alpha(text)\n",
    "    text = case_folding(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bec749",
   "metadata": {},
   "source": [
    "Se agregarán las palabras <SOF> y <EOF> al inicio y al final de cada texto preprocesado, respectivamente para que la última palabra de un correo electrónico no sea relacionada con la primer palabra del siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b888f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in corpus:\n",
    "    text_preprocessed = preprocessing_pipeline(text)\n",
    "    corpus_preprocessed.append(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ab6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [email.split() for email in corpus_preprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c749703",
   "metadata": {},
   "source": [
    "## Construcción de representación TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf642b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebe7f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87528bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aaaaaaaaaaaa', ..., 'zwischen', 'zx', 'zxr'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc87db7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3615, 35198)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6764b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms_for_document(document, amount=5):\n",
    "    row = vectorizer.transform([document])\n",
    "    row = row.tocsr()[0]\n",
    "\n",
    "    order = np.argsort(row.data)[::-1]\n",
    "    top_ids = row.indices[order][:amount]\n",
    "    top_scores = row.data[order][:amount]\n",
    "\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return list(zip(features[top_ids], top_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d65399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(327)\n",
    "doc_indices = random.sample(range(len(corpus_preprocessed)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f39636",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample = [corpus_preprocessed[idx] for idx in doc_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e6c2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documento: wow i hadnt realized how venomous this was getting  be careful herethe problem isnt the rich but the values and the systems that make the rich rich  things are designed in such a way that in order to go with the system and make money everything else we care about goes to shit  i have to constantly remind myself that the goal of human society is not to make money  money doesnt make us happy it just prevents certain things making us more unhappy  therefore dont shoot the rich  shoot the conservatives  drewcifer — top términos TF-IDF:\n",
      "  rich                 0.4764\n",
      "  money                0.2636\n",
      "  make                 0.2499\n",
      "  shoot                0.2198\n",
      "  the                  0.1952\n",
      "\n",
      "Documento:  anas a high rank israeli officer was killed during a clash whith a hamas anas mujahid  the terrorist israelis chased and killed a young mujahid anas using antitank missiles  the terrorist zionists cut the mujahids anas body into small pieces to the extend that his body was not recognized anas at leat ten houses were destroyed by these atnitank missiles  if indeed israeli soldiers killed a hamas mujahid with an antitank missile then im almost sure that the terrorist zionists would not have been able to cut up a body which was probably desintegrated by the missile  stop polluting the net with you fantasies  tsiel — top términos TF-IDF:\n",
      "  anas                 0.5051\n",
      "  mujahid              0.3507\n",
      "  terrorist            0.2324\n",
      "  missiles             0.2020\n",
      "  missile              0.2020\n",
      "\n",
      "Documento: i dont want to attack anyones personal opinions and thus have not included any articles  but it seems to me incredibly ridiculous and pompous for someone to sit back with the benefit of hindsight and point fingers at clinton reno the fbi or whomever  first of all it is a kneejerk judgement  the facts are quite muddled at this point and will likely be for quite a while  secondly things do not improve by pointing blame and accusatory fingers  pointing fingers is a destructive action  if everyone sat around pointing fingers all the time nothing would get done and nothing would ever get any better  and despite the tragedy we can learn something from this  if it is approached in a  constructive manner  doesnt it seem that working together is more productive than working against one another  thirdly it seems incredibly hypocritical to place blame given the benefit of hindsight  something that those who made the decisions did not have the benefit of  why not give them the courtesy of acknowledging that they did the best they could with the data they had  in a very very difficult situation  some responses have gone so far to suggest that the actions were done without regard for the lives of the people in the compound  give me a break  be part of the solution  not the problem  and thats my opinion  — top términos TF-IDF:\n",
      "  fingers              0.3853\n",
      "  the                  0.2584\n",
      "  pointing             0.2260\n",
      "  benefit              0.2175\n",
      "  hindsight            0.2075\n"
     ]
    }
   ],
   "source": [
    "for document in corpus_sample:\n",
    "    tops = get_top_terms_for_document(document, amount=5)\n",
    "\n",
    "    print(f\"\\nDocumento: {document} — top términos TF-IDF:\")\n",
    "    for term, score in tops:\n",
    "        print(f\"  {term:20s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42deecf",
   "metadata": {},
   "source": [
    "### Palabras con mayor peso en algunos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebb77c",
   "metadata": {},
   "source": [
    "**Doc 1**: Los términos reflejan una discusión probablemente sobre riqueza, donde además aparece un término cargado `shoot` que puede cambiar el tono.\n",
    "\n",
    "**Doc 2**: Es evidente que el texto trata sobre terrorismo y conflictos bélicos. Aquí TF-IDF funciona muy bien porque resalta términos muy específicos y técnicos del tema, los cuales difícilmente aparecerán en documentos de autos.\n",
    "\n",
    "**Doc 3**: Aunque son palabras más comunes, muestra cómo TF-IDF puede resaltar vocabulario particular aunque no sea estrictamente técnico, ayudando a identificar la idea principal o el estilo del texto.\n",
    "\n",
    "En general, se observa que TF-IDF prioriza vocabulario distintivo: nombres propios o términos específicos de un conflicto. Además, el alto peso de estas palabras indica que son poco frecuentes en el resto del corpus pero muy relevantes en los documentos en particular donde fueron analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa828d76",
   "metadata": {},
   "source": [
    "### Limitaciones de TF-IDF respecto a la semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f54e5",
   "metadata": {},
   "source": [
    "* **No captura relaciones entre palabras**:\n",
    "Es muy evidente el caso en el **Doc 2** donde aparecen `missile` y `missiles` como términos separados, cuando semánticamente son lo mismo.\n",
    "\n",
    "* **Ignora sinónimos**:\n",
    "Por ejemplo, si en lugar de `money` se usara `cash`, TF-IDF lo trataría como palabra totalmente distinta, sin reconocer que ambos se refieren a lo mismo. Lo mismo con `missiles` y `rockets`.\n",
    "\n",
    "* **No entiende contexto**:\n",
    "El término `shoot` en **Doc 3** podría referirse a disparar, a una sesión de fotos o incluso a una expresión coloquial. TF-IDF no distingue estos significados, solo ve frecuencia.\n",
    "\n",
    "* **Sensibilidad a términos comunes**:\n",
    "Palabras como `the` aparecen en los tops porque, aunque muy frecuentes en general, pueden quedar con peso significativo si en ese documento tienen una proporción mayor que en otros. Esto puede “ensuciar” el análisis semántico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddebbdb",
   "metadata": {},
   "source": [
    "## Construcción de representación PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63868e",
   "metadata": {},
   "source": [
    "La implementación de ambos métodos se basó en una discusión de **Stack Overflow**: https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77194e80",
   "metadata": {},
   "source": [
    "Para los resultados, se muestra los mismos 3 documentos analizados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2588c6",
   "metadata": {},
   "source": [
    "### Construya una matriz de co-ocurrencia palabra-contexto con ventana deslizante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bbfb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_df(sentences, window_size=2, top_k=800):\n",
    "    tokenized_docs = [s.split() for s in sentences]\n",
    "    unigrams = Counter()\n",
    "    for toks in tokenized_docs:\n",
    "        unigrams.update([t for t in toks])\n",
    "    vocab = [w for w,_ in unigrams.most_common(top_k)]\n",
    "    vocab_set = set(vocab)\n",
    "\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for toks in tokenized_docs:\n",
    "        n = len(toks)\n",
    "        for i, w in enumerate(toks):\n",
    "            if w not in vocab_set:\n",
    "                continue\n",
    "            L = max(0, i - window_size)\n",
    "            R = min(n, i + window_size + 1)\n",
    "            ctx = toks[L:i] + toks[i+1:R]\n",
    "            for c in ctx:\n",
    "                if c in vocab_set:\n",
    "                    co_counts[w][c] += 1\n",
    "\n",
    "    df = pd.DataFrame(0, index=vocab, columns=vocab, dtype=np.int32)\n",
    "    for w, row in co_counts.items():\n",
    "        for c, cnt in row.items():\n",
    "            df.at[w, c] = cnt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "088e5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence(sentences, window_size):\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        # preprocessing (use tokenizer instead)\n",
    "        text = text.lower().split()\n",
    "        # iterate over sentences\n",
    "        for i in range(len(text)):\n",
    "            token = text[i]\n",
    "            vocab.add(token)  # add to vocab\n",
    "            next_token = text[i+1 : i+1+window_size]\n",
    "            for t in next_token:\n",
    "                key = tuple( sorted([t, token]) )\n",
    "                d[key] += 1\n",
    "    \n",
    "    # formulate the dictionary into dataframe\n",
    "    vocab = sorted(vocab) # sort vocab\n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d290aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix = co_occurrence(corpus_sample[:1], window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ec63d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>about</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>be</th>\n",
       "      <th>but</th>\n",
       "      <th>care</th>\n",
       "      <th>careful</th>\n",
       "      <th>certain</th>\n",
       "      <th>conservatives</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>us</th>\n",
       "      <th>values</th>\n",
       "      <th>venomous</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wow</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a  about  and  are  be  but  care  careful  certain  conservatives  \\\n",
       "a      0      0    0    0   0    0     0        0        0              0   \n",
       "about  0      0    0    0   0    0     1        0        0              0   \n",
       "and    0      0    0    0   0    0     0        0        0              0   \n",
       "are    0      0    0    0   0    0     0        0        0              0   \n",
       "be     0      0    0    0   0    0     0        1        0              0   \n",
       "...   ..    ...  ...  ...  ..  ...   ...      ...      ...            ...   \n",
       "was    0      0    0    0   1    0     0        0        0              0   \n",
       "way    1      0    0    0   0    0     0        0        0              0   \n",
       "we     0      1    0    0   0    0     1        0        0              0   \n",
       "with   0      0    0    0   0    0     0        0        0              0   \n",
       "wow    0      0    0    0   0    0     0        0        0              0   \n",
       "\n",
       "       ...  to  unhappy  us  values  venomous  was  way  we  with  wow  \n",
       "a      ...   0        0   0       0         0    0    1   0     0    0  \n",
       "about  ...   1        0   0       0         0    0    0   1     0    0  \n",
       "and    ...   0        0   0       1         0    0    0   0     0    0  \n",
       "are    ...   0        0   0       0         0    0    0   0     0    0  \n",
       "be     ...   0        0   0       0         0    1    0   0     0    0  \n",
       "...    ...  ..      ...  ..     ...       ...  ...  ...  ..   ...  ...  \n",
       "was    ...   0        0   0       0         1    0    0   0     0    0  \n",
       "way    ...   0        0   0       0         0    0    0   0     0    0  \n",
       "we     ...   0        0   0       0         0    0    0   0     0    0  \n",
       "with   ...   1        0   0       0         0    0    0   0     0    0  \n",
       "wow    ...   0        0   0       0         0    0    0   0     0    0  \n",
       "\n",
       "[67 rows x 67 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f9e33",
   "metadata": {},
   "source": [
    "### Calcule la matriz PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f823e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ppmi(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    row_tot = df.sum(axis=1).astype(float)\n",
    "    col_tot = df.sum(axis=0).astype(float)\n",
    "    total = col_tot.sum()\n",
    "\n",
    "    expected = np.outer(row_tot.values, col_tot.values) / (total if total>0 else 1.0)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        ratio = df.values / expected\n",
    "        pmi = np.log2(ratio, where=(ratio>0))\n",
    "    pmi[~np.isfinite(pmi)] = 0.0\n",
    "    pmi[pmi < 0] = 0.0\n",
    "    return pd.DataFrame(pmi, index=df.index, columns=df.columns, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c98b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(df, positive=True):\n",
    "    col_totals = df.sum(axis=0)\n",
    "    total = col_totals.sum()\n",
    "    row_totals = df.sum(axis=1)\n",
    "    expected = np.outer(row_totals, col_totals) / total\n",
    "    df = df / expected\n",
    "    # Silence distracting warnings about log(0):\n",
    "    with np.errstate(divide='ignore'):\n",
    "        df = np.log(df)\n",
    "    df[np.isinf(df)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        df[df < 0] = 0.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdd78110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi = pmi(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1635d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>about</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>be</th>\n",
       "      <th>but</th>\n",
       "      <th>care</th>\n",
       "      <th>careful</th>\n",
       "      <th>certain</th>\n",
       "      <th>conservatives</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>us</th>\n",
       "      <th>values</th>\n",
       "      <th>venomous</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.738271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.431418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.124565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.738271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wow</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              a     about  and  are        be  but      care   careful  \\\n",
       "a      0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "about  0.000000  0.000000  0.0  0.0  0.000000  0.0  3.124565  0.000000   \n",
       "and    0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "are    0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "be     0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  3.124565   \n",
       "...         ...       ...  ...  ...       ...  ...       ...       ...   \n",
       "was    0.000000  0.000000  0.0  0.0  3.124565  0.0  0.000000  0.000000   \n",
       "way    3.124565  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "we     0.000000  3.124565  0.0  0.0  0.000000  0.0  3.124565  0.000000   \n",
       "with   0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "wow    0.000000  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "       certain  conservatives  ...        to  unhappy   us    values  \\\n",
       "a          0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "about      0.0            0.0  ...  1.738271      0.0  0.0  0.000000   \n",
       "and        0.0            0.0  ...  0.000000      0.0  0.0  2.431418   \n",
       "are        0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "be         0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "...        ...            ...  ...       ...      ...  ...       ...   \n",
       "was        0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "way        0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "we         0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "with       0.0            0.0  ...  1.738271      0.0  0.0  0.000000   \n",
       "wow        0.0            0.0  ...  0.000000      0.0  0.0  0.000000   \n",
       "\n",
       "       venomous       was       way        we  with  wow  \n",
       "a      0.000000  0.000000  3.124565  0.000000   0.0  0.0  \n",
       "about  0.000000  0.000000  0.000000  3.124565   0.0  0.0  \n",
       "and    0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "are    0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "be     0.000000  3.124565  0.000000  0.000000   0.0  0.0  \n",
       "...         ...       ...       ...       ...   ...  ...  \n",
       "was    3.124565  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "way    0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "we     0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "with   0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "wow    0.000000  0.000000  0.000000  0.000000   0.0  0.0  \n",
       "\n",
       "[67 rows x 67 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78119a8",
   "metadata": {},
   "source": [
    "### Discuta ventajas y desventajas de PPMI respecto a TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922232e",
   "metadata": {},
   "source": [
    "#### Ventajas \n",
    "\n",
    "- PPMI captura mejor las relaciones semánticas entre palabras al considerar la co-ocurrencia en un contexto más amplio.\n",
    "- PPMI puede resaltar términos que son informativos en contextos específicos dada la frecuencia de co-ocurrencia.\n",
    "- PPMI es menos sensible a la frecuencia de documentos y más a la estructura de co-ocurrencia, lo que puede ser beneficioso en ciertos análisis semánticos.\n",
    "\n",
    "\n",
    "#### Desventajas\n",
    "\n",
    "- PPMI puede ser computacionalmente más costoso que TF-IDF, especialmente en grandes corpus de texto.\n",
    "- PPMI puede generar matrices dispersas y de alta dimensión, lo que puede dificultar su manejo y análisis.\n",
    "- PPMI no considera la frecuencia de los términos en el documento, lo que puede ser una limitación en ciertos contextos de recuperación de información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60a60f",
   "metadata": {},
   "source": [
    "## Construcción de representación Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0290a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "    sentences=tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cc2fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540515ee",
   "metadata": {},
   "source": [
    "### Palabras más cercanas y lejanas en el espacio vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8edfa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_words(word, topn=10):\n",
    "    if word not in wv:\n",
    "        return []\n",
    "    return wv.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e29d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_similar_words(word, topn=10, search_top_k=5000):\n",
    "    if word not in wv:\n",
    "        return []\n",
    "    vocab = list(wv.key_to_index.keys())[:search_top_k]\n",
    "    v = wv[word] / np.linalg.norm(wv[word])\n",
    "    sims = []\n",
    "    for w in vocab:\n",
    "        if w == word: \n",
    "            continue\n",
    "        u = wv[w] / np.linalg.norm(wv[w])\n",
    "        sims.append((w, float(np.dot(v, u))))\n",
    "    sims.sort(key=lambda x: x[1])\n",
    "    return sims[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d15a46e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Más cercanas a 'car':\n",
      "  price            0.853\n",
      "  engine           0.841\n",
      "  driver           0.833\n",
      "  little           0.822\n",
      "  delorean         0.819\n",
      "  dealership       0.815\n",
      "  oil              0.815\n",
      "  post             0.814\n",
      "\n",
      "Más lejanas a 'car':\n",
      "  turkish          -0.179\n",
      "  by               -0.173\n",
      "  armenian         -0.162\n",
      "  muslim           -0.129\n",
      "  against          -0.119\n",
      "  kurdish          -0.089\n",
      "  exterminate      -0.086\n",
      "  reparations      -0.069\n",
      "\n",
      "Más cercanas a 'government':\n",
      "  force            0.855\n",
      "  citizens         0.855\n",
      "  israel           0.853\n",
      "  serbians         0.837\n",
      "  irish            0.834\n",
      "  society          0.825\n",
      "  federal          0.825\n",
      "  land             0.822\n",
      "\n",
      "Más lejanas a 'government':\n",
      "  my               -0.105\n",
      "  edt              0.014\n",
      "  her              0.032\n",
      "  am               0.035\n",
      "  sadikov          0.044\n",
      "  briefing         0.047\n",
      "  ago              0.050\n",
      "  v                0.065\n",
      "\n",
      "Más cercanas a 'gun':\n",
      "  crime            0.848\n",
      "  laws             0.846\n",
      "  choice           0.840\n",
      "  public           0.837\n",
      "  system           0.832\n",
      "  control          0.831\n",
      "  private          0.829\n",
      "  necessary        0.828\n",
      "\n",
      "Más lejanas a 'gun':\n",
      "  sadikov          -0.049\n",
      "  rasulov          -0.011\n",
      "  his              -0.001\n",
      "  edt              0.022\n",
      "  said             0.022\n",
      "  her              0.026\n",
      "  was              0.041\n",
      "  told             0.049\n",
      "\n",
      "Más cercanas a 'israel':\n",
      "  citizens         0.870\n",
      "  government       0.853\n",
      "  arabs            0.841\n",
      "  palestinians     0.831\n",
      "  peace            0.822\n",
      "  discriminate     0.813\n",
      "  weapons          0.810\n",
      "  force            0.809\n",
      "\n",
      "Más lejanas a 'israel':\n",
      "  edt              -0.069\n",
      "  wallet           0.012\n",
      "  v                0.027\n",
      "  at               0.042\n",
      "  into             0.062\n",
      "  tells            0.070\n",
      "  sadikov          0.082\n",
      "  adamdasharvardedu  0.087\n",
      "\n",
      "Más cercanas a 'ford':\n",
      "  tires            0.965\n",
      "  saturn           0.963\n",
      "  reliability      0.960\n",
      "  transmission     0.958\n",
      "  tbird            0.955\n",
      "  alarm            0.952\n",
      "  toyota           0.951\n",
      "  cop              0.950\n",
      "\n",
      "Más lejanas a 'ford':\n",
      "  are              0.174\n",
      "  turkish          0.185\n",
      "  were             0.194\n",
      "  against          0.211\n",
      "  armenian         0.217\n",
      "  armenians        0.232\n",
      "  will             0.243\n",
      "  by               0.253\n"
     ]
    }
   ],
   "source": [
    "for w in [\"car\", \"government\", \"gun\", \"israel\", \"ford\"]:\n",
    "        print(f\"\\nMás cercanas a '{w}':\")\n",
    "        for t, s in most_similar_words(w, topn=8):\n",
    "            print(f\"  {t:15s}  {s:.3f}\")\n",
    "        print(f\"\\nMás lejanas a '{w}':\")\n",
    "        for t, s in least_similar_words(w, topn=8, search_top_k=5000):\n",
    "            print(f\"  {t:15s}  {s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf2292",
   "metadata": {},
   "source": [
    "### Diferencias con TF-IDF y PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a02c10",
   "metadata": {},
   "source": [
    "#### Word2Vec vs TF-IDF\n",
    "\n",
    "**Word2Vec** como vimos en clase, aprende vectores densos y continuos que capturan la semántica por contexto. Dos palabras sin superposición léxica pueden quedar cerca `car` y `automobile`.\n",
    "\n",
    "**TF-IDF** es disperso y léxico: mide importancia de términos por documento, no entiende sinónimos ni contexto.\n",
    "\n",
    "#### Word2Vec vs PPMI\n",
    "\n",
    "**PPMI** ya capta asociación estadística de co-ocurrencias, pero en una matriz enorme |V|×|V| y sin aprendizaje paramétrico.\n",
    "\n",
    "**Word2Vec** puede verse como una compresión no lineal de co-ocurrencias que produce embeddings compactos y generalizables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac655ff4",
   "metadata": {},
   "source": [
    "## Evaluación comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4b416e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.target\n",
    "target_names = dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ace59da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
    "    corpus_preprocessed, y, test_size=0.2, random_state=327, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53e505",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28290c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF accuracy: 0.7939\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train_txt)\n",
    "X_test_tfidf  = vectorizer.transform(X_test_txt)\n",
    "\n",
    "clf_tfidf = LogisticRegression(max_iter=1000, n_jobs=None)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "acc_tfidf = accuracy_score(y_test, pred_tfidf)\n",
    "print(f\"TF-IDF accuracy: {acc_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641255d",
   "metadata": {},
   "source": [
    "#### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cddcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_from_word_matrix(word_matrix: pd.DataFrame, docs_tokens):\n",
    "    idx = word_matrix.index\n",
    "    word2row = {w:i for i,w in enumerate(idx)}\n",
    "    W = word_matrix.values\n",
    "    D = []\n",
    "    for toks in docs_tokens:\n",
    "        rows = [word2row[w] for w in toks if w in word2row]\n",
    "        if rows:\n",
    "            D.append(W[rows].mean(axis=0))\n",
    "        else:\n",
    "            D.append(np.zeros(W.shape[1], dtype=np.float32))\n",
    "    return np.vstack(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9993a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_cooc = co_occurrence_df(X_train_txt, window_size=2, top_k=800)\n",
    "ppmi_mat = to_ppmi(ppmi_cooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d039e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toks = [s.split() for s in X_train_txt]\n",
    "X_test_toks  = [s.split() for s in X_test_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc88225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI (avg word vectors) accuracy: 0.7012\n"
     ]
    }
   ],
   "source": [
    "X_train_ppmi = docs_from_word_matrix(ppmi_mat, X_train_toks)\n",
    "X_test_ppmi  = docs_from_word_matrix(ppmi_mat, X_test_toks)\n",
    "\n",
    "clf_ppmi = LogisticRegression(max_iter=1000)\n",
    "clf_ppmi.fit(X_train_ppmi, y_train)\n",
    "pred_ppmi = clf_ppmi.predict(X_test_ppmi)\n",
    "acc_ppmi = accuracy_score(y_test, pred_ppmi)\n",
    "print(f\"PPMI (avg word vectors) accuracy: {acc_ppmi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725af1d6",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "781f22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = [s.split() for s in X_train_txt]\n",
    "w2v_cls = Word2Vec(\n",
    "    sentences=tokens_train,\n",
    "    vector_size=100, window=5,\n",
    "    min_count=2, sg=1, epochs=10, workers=4\n",
    ")\n",
    "wv_cls = w2v_cls.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1468b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_avg_w2v(tokens, wv):\n",
    "    vecs = [wv[w] for w in tokens if w in wv]\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    return np.zeros(wv.vector_size, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcf26898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = np.vstack([doc_avg_w2v(t, wv_cls) for t in tokens_train])\n",
    "X_test_w2v  = np.vstack([doc_avg_w2v(s.split(), wv_cls) for s in X_test_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f309f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec (avg word vectors) accuracy: 0.7607\n"
     ]
    }
   ],
   "source": [
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "pred_w2v = clf_w2v.predict(X_test_w2v)\n",
    "acc_w2v = accuracy_score(y_test, pred_w2v)\n",
    "print(f\"Word2Vec (avg word vectors) accuracy: {acc_w2v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee0c8f",
   "metadata": {},
   "source": [
    "### Tabla comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "290b48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Representación</th>\n",
       "      <th>Precisión</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.793914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPMI</td>\n",
       "      <td>0.701245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.760719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Representación  Precisión\n",
       "0         TF-IDF   0.793914\n",
       "1           PPMI   0.701245\n",
       "2       Word2Vec   0.760719"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Representación\": [\"TF-IDF\", \"PPMI\", \"Word2Vec\"],\n",
    "    \"Precisión\": [acc_tfidf, acc_ppmi, acc_w2v]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec39339",
   "metadata": {},
   "source": [
    "Con base en los resultados obtenidos, se observa que **TF-IDF** alcanzó la mayor precisión **(79.39%)**, lo cual es consistente con su fortaleza en tareas de clasificación de documentos. Por otro lado, **Word2Vec** obtuvo un **76.07%**, mostrando que los embeddings capturan relaciones semánticas útiles, pero al promediar los vectores de palabras se pierde parte de la estructura contextual. Finalmente, **PPMI** alcanzó un **70.12%**, ya que si bien resalta asociaciones de co-ocurrencia entre palabras, su alta dimensionalidad y dispersión reducen su efectividad práctica en clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c80749",
   "metadata": {},
   "source": [
    "## Discusión final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfadb9e",
   "metadata": {},
   "source": [
    "### Cómo cada representación captura (o no) relaciones semánticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924128bf",
   "metadata": {},
   "source": [
    "**TF-IDF** como se pudo ver en todo el laboratorio se basa solo en la frecuencia de términos  no captura relaciones semánticas. **PPMI**, en cambio, sí tiene cierta semántica al resaltar asociaciones entre palabras que tienden a aparecer en contextos similares, lo que permite identificar relaciones de co-ocurrencia significativas. Y sobre todo **Word2Vec**  aprende representaciones densas y continuas donde palabras con contextos parecidos ocupan posiciones cercanas en el espacio vectorial, lo que le permite capturar sinónimos, analogías y similitud semántica de forma más natural que los otros dos enfoques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda60be",
   "metadata": {},
   "source": [
    "### Escenarios donde cada técnica es más útil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee6170",
   "metadata": {},
   "source": [
    "**TF-IDF** resulta muy útil en tareas de clasificación de documentos y recuperación de información, donde lo importante es identificar términos relevantes. **PPMI** se ajusta mejor a estudios semánticos, ya que su matriz muestra las asociaciones entre palabras y permite explorar patrones de co-ocurrencia. **Word2Vec** es ideal para tareas que requieren capturar similitud y relaciones semánticas profundas, como búsqueda semántica, detección de sinónimos, análisis de sentimientos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011474f0",
   "metadata": {},
   "source": [
    "### Limitaciones prácticas (memoria, tiempo de cómputo, interpretabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4976e",
   "metadata": {},
   "source": [
    "**TF-IDF** es eficiente y fácil de interpretar, pero genera vectores dispersos de alta dimensión y carece de semántica. **PPMI** como se puede ver en los tiempos de ejecución, presenta el mayor reto práctico: la matriz de co-ocurrencias es de tamaño |V|×|V| (donde |V| es el vocabulario), lo que puede volverse costoso en corpus grandes. **Word2Vec**, aunque produce embeddings densos y compactos, requiere más tiempo de entrenamiento y ajustes de hiperparámetros, y sus representaciones no son directamente interpretables, lo que dificulta explicar por qué dos palabras aparecen cercanas en el espacio vectorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
